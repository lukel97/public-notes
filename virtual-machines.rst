Safepoints and Checkpoints are Yield Points
--------------------------------------------

I recently read an `interesting survey of GC API design alternative available for Rust <https://manishearth.github.io/blog/2021/04/05/a-tour-of-safe-tracing-gc-designs-in-rust/>`_, and it included a comment about the duality of async and garbage collection safepoints which got me thinking.

Language runtimes are full of situations where one thread needs to ask another to perform some action on its behalf.  Consider the following examples:

* Assumption invalidation - When a speculative assumption has been violated, a runtime needs to evict all running threads from previously compiled code which makes the about to be invalidated assumption.  To do so, it must arrange for all other threads to remove the given piece of compiled code from it's execution stack.  
* Locking protocols - It is common to optimize the case where only a single thread is interacting with a locked object.  In order for another thread to interact with the lock, it may need the currently owning thread to perform a compensation action on it's default.
* Garbage collection - The garbage collector needs the running mutator threads to scan their stacks and report a list of garbage collected objects which are rooted by the given thread.
* Debuggers and profilers - Tools frequently need to stop a thread and ask it to report information about it's thread context.  Depending on the information required, this may be possible without (much) cooperation from the running thread, but the more involved the information, the more likely we need the queried thread to be in a known state or otherwise cooperate with the execution of the query.  

Interestingly, these are all forms of cooperative thread preemption (i.e. cooperative multi-threading).  The currently running task is interrupted, another task is scheduled, and then the original task is rescheduled once the interrupting task is complete.  (To avoid confusion, let's be explicit about the fact that it's semantically the *abstract machine thread* being interrupted and resumed.  The physical execution execution may look quite different once abstract machine execution resumes for the interrupted thread.)

Beyond these preemption examples, there are also a number of cases where a single thread needs to transition from one physical execution context to another.  Conceptually, these transitions don't change the abstract machine state of the running thread, but we can consider them premption points as well by modeling the transition code which switches from one physical execution context to another as being another conceptual thread.  

Consider the "on stack replacement" and "uncommon trap" or "side exit" transitions.  The former transitions a piece of code from running an interpreter to a compiled piece of code, the later does the inverse.  There's usually a non-trivial amount of VM code which runs between the two pieces of abstract machine execution to do e.g. argument marshalling and frame setup.  We can consider there to be two conceptual threads: the abstract machine thread, and the "transition thread" which is trying to transition the code from one mode of execution to another.  The abstract machine thread reaches a logical premption point, transitions control to the transition thread, and then the transition thread returns control to the abstract machine thread (but running in another physical tier of exeuction.)

It is worth highlighting that while this is cooperative premption, it is not *generalized* cooperative premption.  That is, the code being transitions to at a premption point is not arbitrary.  In fact, there are usually very strong semantic restrictions on what it can do.  This restricted semantics allows the generated code from an optimized compiler to be optimized around these potential premption points in interesting ways.

It is worth noting that (at least in theory) the abstract machine thread may have different sets of premption points for each class of prempting thread.  (Said differently, maybe not all of your lock protocol premption points allow general deoptimization, or maybe your GC safepoints don't allow lock manipulation.)  This is quite difficult to take advantage of in practice - mostly because maintaining any form of timeliness guarantee gets complicated if you have unbounded prempting tasks and don't have the ability to prempt them in turn - but at least in theory the flexibility exists.

This observation raises some interesting possibilities for implementing safepoints and checkpoints in a compiler.  There's a lot of work on compiling continuations and generators, I wonder if anyone has explored what falls out if we view a safepoint as just another form of yield point?  Thinking through how you might level CPS style optimization tricks in such a model would be quite interesting.  (This may have already been explored in the academic literature, CPS compilation isn't an area I follow closely.)  
